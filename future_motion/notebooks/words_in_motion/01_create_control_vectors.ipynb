{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_project_root():\n",
    "    # get the absolute path to the root of the git repo\n",
    "    root = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"]).strip().decode(\"utf-8\")\n",
    "    return Path(root)\n",
    "\n",
    "# get project root and append it to path\n",
    "project_root = get_project_root()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# embeddings path\n",
    "dataset = \"waymo\"\n",
    "data_dir = f\"{dataset}_data\"\n",
    "base_path = os.path.normpath(os.path.join(project_root, \"..\"))\n",
    "\n",
    "# output dir\n",
    "out_reldir = f\"out/control-vectors/{dataset}/\"\n",
    "out_path = os.path.join(base_path, out_reldir)\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load target embeddings; torch.Size([48, 11, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 454.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from utils.embs_all import load_embeddings\n",
    "from utils.embs_contrastive import load_contrastive_embed_pairs\n",
    "\n",
    "\n",
    "# load data\n",
    "data_path = os.path.join(base_path, \"data\", data_dir)\n",
    "paths_inputs = sorted(glob(f\"{data_path}/input*\"))\n",
    "paths_embeds = sorted(glob(f\"{data_path}/target_embs*\"))\n",
    "\n",
    "# stack embeddings wrt types\n",
    "embs = load_embeddings(paths_inputs, paths_embeds)\n",
    "\n",
    "# trim and stack contrastive pairs of embeddings\n",
    "contrastive_embs = load_contrastive_embed_pairs(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 1996.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(204, torch.Size([48, 11, 128]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# get all embeddings\n",
    "embs_all = []\n",
    "for path_embs in tqdm(paths_embeds):\n",
    "    embs = torch.load(path_embs, map_location=torch.device('cpu'))\n",
    "    embs_all.append(embs)\n",
    "\n",
    "# we have 3 modules; hence 3 hidden states in embs_all\n",
    "# 48 batch size, 11 past (Waymo), 128 hidden state size\n",
    "len(embs_all), embs_all[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "204it [00:00, 2438.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# get the last module's hidden state\n",
    "embs_all_last = []\n",
    "for (path_input, path_embs) in tqdm(zip(paths_inputs, paths_embeds)):\n",
    "    embs = torch.load(path_embs, map_location=torch.device('cpu'))\n",
    "    # take the last module (layer) and the last past time step (the current one) of it\n",
    "    embs_all_last.append(embs[-1][:, -1])\n",
    "embs_all_last = torch.stack(embs_all_last, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(embs_all_last, embs_all_last)  # Autoencoder output = input\n",
    "# set 'drop_last=False' to get the exact the same loss values as in the paper\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=False, num_workers=15)  # Adjust batch size as needed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create control vectors using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from future_motion.utils.interpretability.control_vectors import fit_control_vector\n",
    "\n",
    "\n",
    "idx_layer = 2\n",
    "PCA_control_vectors = {}\n",
    "for key in contrastive_embs.keys():\n",
    "    PCA_control_vectors[key] = fit_control_vector(contrastive_embs[key])\n",
    "    torch.save(torch.tensor(PCA_control_vectors[key]), f=f\"{out_path}/pca_{key}_layer{idx_layer}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create control vectors using SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 33.0 K | n/a \n",
      "---------------------------------------------\n",
      "33.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.0 K    Total params\n",
      "0.132     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/tas/.virtualenvs/words/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9999: 100%|██████████| 7/7 [00:00<00:00, 23.34it/s, v_num=0, loss=5.740]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9999: 100%|██████████| 7/7 [00:00<00:00, 23.02it/s, v_num=0, loss=5.740]\n",
      "Best model loaded from /home/tas/00_workspaces/words_in_motion/out/control-vectors/waymo/checkpoints/sae128-epoch=9435-loss=4.3303.ckpt\n",
      "Best loss: 4.330253\n",
      "Best model saved to /home/tas/00_workspaces/words_in_motion/out/control-vectors/waymo/sae_waymo_n128.pth\n"
     ]
    }
   ],
   "source": [
    "from future_motion.utils.interpretability.sparse_autoencoder import SparseAutoencoder, SAE\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "autoencoders = {}\n",
    "d_mlp = 128\n",
    "for n_hidden in [512, 256, 128, 64, 32, 16]:\n",
    "\n",
    "    # Create specific subdirectories under out_path\n",
    "    log_dir = os.path.join(out_path, \"logs\")\n",
    "    checkpoint_dir = os.path.join(out_path, \"checkpoints\")\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    model = SAE(d_mlp, n_hidden, max_epochs=10000)\n",
    "\n",
    "    logger = TensorBoardLogger(log_dir, name=\"sae_training\")\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"loss\",\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f\"sae{n_hidden}-\" + \"{epoch:02d}-{loss:.4f}\",\n",
    "        save_top_k=3,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "\n",
    "    # Create the trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10000,\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader)\n",
    "\n",
    "    # Only load and save the checkpoint on the main process (global_rank == 0)\n",
    "    if trainer.global_rank == 0:\n",
    "        # Load the best checkpoint\n",
    "        best_model_path = checkpoint_callback.best_model_path\n",
    "        best_model = SAE.load_from_checkpoint(best_model_path)\n",
    "        autoencoders[n_hidden] = best_model\n",
    "\n",
    "        print(f\"Best model loaded from {best_model_path}\")\n",
    "        print(f\"Best loss: {checkpoint_callback.best_model_score.item():.6f}\")\n",
    "\n",
    "        # Save model to out_path\n",
    "        model_save_path = os.path.join(out_path, f\"sae_waymo_n{n_hidden}.pth\")\n",
    "        torch.save(best_model.state_dict(), model_save_path)\n",
    "        print(f\"Best model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for 10.000 epochs - SAE\n",
    "(Google Colab T4-instance; seed not set / cuda=12.5 / torch=2.6.0+cu124 / sklearn=2.0.2 / python=3.11.11 (final))\n",
    "\n",
    "| Hidden Dim | Epoch         | Total Loss   | L1 Loss       | L2 Loss        | Total Reconst Loss |\n",
    "|------------|---------------|--------------|---------------|----------------|--------------------|\n",
    "| 512        | 9805/10000    | 4.005656276  | 1.524447083   | 8270.697265625 | 0.001645120210014  |\n",
    "| 256        | 9845/10000    | 3.724161590  | 1.376968503   | 7823.977050781 | 0.001388887991197  |\n",
    "| 128        | 9820/10000    | 4.139010770  | 1.556326985   | 8608.9453125   | 0.001653907238506  |\n",
    "| 64         | 9348/10000    | 4.561335734  | 1.892843366   | 8894.974609375 | 0.001926084747538  |\n",
    "| 32         | 9864/10000    | 7.141473430  | 3.902811527   | 10795.541015625| 0.004311752039939  |\n",
    "| 16         | 9956/10000    | 17.441959654 | 13.368986130  | 13576.573242188| 0.014228038489819  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed\n",
      "explained variance: [0.66795915 0.0779107  0.05768754 0.03374871 0.02376009 0.01854349\n",
      " 0.01611649 0.01115513 0.01029056 0.00726391]\n",
      "acceleration\n",
      "explained variance: [0.63380283 0.08891191 0.08114744 0.03292668 0.02905993 0.02080279\n",
      " 0.01766137 0.01140689 0.00912073 0.00772376]\n",
      "direction\n",
      "explained variance: [0.40223372 0.13826407 0.10420155 0.07386495 0.04946833 0.03335578\n",
      " 0.02050195 0.01822362 0.01589583 0.01390625]\n",
      "agent\n",
      "explained variance: [0.59231377 0.10558699 0.0751505  0.03179755 0.02696528 0.02449778\n",
      " 0.02359529 0.01325316 0.01098    0.00962443]\n",
      "speed\n",
      "explained variance: [0.68807936 0.073489   0.0685495  0.02807502 0.02159631 0.01922441\n",
      " 0.0177188  0.00981717 0.00746597 0.00639294]\n",
      "acceleration\n",
      "explained variance: [0.63591397 0.098262   0.08801161 0.03279012 0.02852907 0.02120169\n",
      " 0.01530502 0.01093495 0.00692878 0.00642024]\n",
      "direction\n",
      "explained variance: [0.40637618 0.14300221 0.11172867 0.08087072 0.04480727 0.03594213\n",
      " 0.02242988 0.01866051 0.01497646 0.01319762]\n",
      "agent\n",
      "explained variance: [0.60484153 0.11244339 0.08834464 0.03189723 0.02394665 0.02084277\n",
      " 0.01960899 0.01387739 0.00866841 0.0077735 ]\n",
      "speed\n",
      "explained variance: [0.7069722  0.07533547 0.06053196 0.02618496 0.02245519 0.01962931\n",
      " 0.01658416 0.01014226 0.00624328 0.00501995]\n",
      "acceleration\n",
      "explained variance: [0.6448377  0.09366593 0.08813526 0.03922399 0.02615505 0.01988537\n",
      " 0.01537103 0.0112698  0.00615829 0.00576678]\n",
      "direction\n",
      "explained variance: [0.41152975 0.14129753 0.12266462 0.07228874 0.05059392 0.04127079\n",
      " 0.02599148 0.01419549 0.01336266 0.01021152]\n",
      "agent\n",
      "explained variance: [0.64352363 0.08859315 0.08291098 0.0344568  0.02296875 0.02048467\n",
      " 0.01710406 0.01224618 0.00893854 0.00705127]\n",
      "speed\n",
      "explained variance: [0.7092707  0.07612746 0.06333381 0.03104858 0.02228557 0.01537084\n",
      " 0.0104264  0.00846576 0.0081314  0.00725549]\n",
      "acceleration\n",
      "explained variance: [0.67234194 0.09518757 0.07575919 0.03683547 0.0268645  0.01578252\n",
      " 0.01225242 0.00823079 0.0077345  0.00577776]\n",
      "direction\n",
      "explained variance: [0.44027662 0.17046255 0.10139287 0.07505573 0.03496922 0.03281816\n",
      " 0.0208252  0.01716027 0.01240012 0.01080253]\n",
      "agent\n",
      "explained variance: [0.6514714  0.09534302 0.08418901 0.03518938 0.0267825  0.01849964\n",
      " 0.01138099 0.00943867 0.00833492 0.00709693]\n",
      "speed\n",
      "explained variance: [0.76621544 0.08249394 0.04016063 0.02329861 0.01840852 0.01517915\n",
      " 0.01114475 0.00796367 0.00605518 0.00405969]\n",
      "acceleration\n",
      "explained variance: [0.7223367  0.11251053 0.04810891 0.02481488 0.02255747 0.01091234\n",
      " 0.00989002 0.00874757 0.00787679 0.00469149]\n",
      "direction\n",
      "explained variance: [0.4864162  0.1302546  0.12223336 0.08628114 0.03588887 0.02856426\n",
      " 0.01864577 0.01631941 0.01354782 0.01134494]\n",
      "agent\n",
      "explained variance: [0.6899109  0.09814776 0.06133359 0.03935973 0.02810579 0.01589964\n",
      " 0.01299426 0.009389   0.00728279 0.00671909]\n",
      "speed\n",
      "explained variance: [0.7264958  0.08313958 0.04593519 0.04363234 0.01999024 0.01789235\n",
      " 0.01376111 0.01052237 0.00909336 0.00719666]\n",
      "acceleration\n",
      "explained variance: [0.69389665 0.09072881 0.05742767 0.05119225 0.02569301 0.01834447\n",
      " 0.01625383 0.01233639 0.00861877 0.00695004]\n",
      "direction\n",
      "explained variance: [0.3385522  0.23896188 0.1352084  0.06464925 0.05112161 0.03658743\n",
      " 0.03139723 0.02447643 0.02063072 0.01963937]\n",
      "agent\n",
      "explained variance: [0.6024382  0.14666572 0.07171694 0.04700173 0.032978   0.02728518\n",
      " 0.01634489 0.01447157 0.01009368 0.00840261]\n"
     ]
    }
   ],
   "source": [
    "from future_motion.utils.interpretability.control_vectors import fit_control_vector\n",
    "\n",
    "\n",
    "# Create control vectors with SAE\n",
    "SAE_control_vectors = {}\n",
    "for hidden_dims, autoencoder in autoencoders.items():\n",
    "    SAE_control_vectors[hidden_dims] = dict()\n",
    "    for key in contrastive_embs.keys():\n",
    "        print(key)\n",
    "        cv = fit_control_vector(contrastive_embs[key], autoencoder=autoencoder, verbose_explained_variance=True)\n",
    "        SAE_control_vectors[hidden_dims][key] = cv\n",
    "        torch.save(torch.tensor(cv), f=f\"{out_path}/sae{hidden_dims}_{key}_layer{idx_layer}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "words",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
